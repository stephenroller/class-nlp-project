\documentclass{article}

\title{CS388 Final Project Writeup:\\ 
Unknown Noun Supersense Acquisition using Search Engines}

\author{Stephen Roller \& Karl Pichotta}

\usepackage{hyperref}


\begin{document}

\maketitle




\section{Introduction}

%Motivate and abstractly describe the problem you are addressing and how you are addressing it. What is the problem? Why is it important? What is your basic approach? A short discussion of how it fits into related work in the area is also desirable. Summarize the basic results and conclusions that you will present. 

We describe a novel method for tagging unknown nouns with supersenses using Search Engine queries to refine an initial precomputed approximation.
A noun's {\em supersense} is the name given in the literature to highest meaningful level of meaning in the WordNet \cite{wordnet} hierarchy---these are abstract concepts such as {\it food}, {\it person}, or {\it cognition} \cite{cj}.

We provide a novel solution to the problem which uses a model built from a relatively small offline corpus to provide a first approximation, and then further refines this approximation by querying a search engine.
This broad approach---use a rough model to construct an approximation and refine with search engine queries---could be generalized to many other tasks.
In that sense, our contribution is not an algorithm to solve the problem of acquisition of supersenses of unknown nouns {\it per se}, but is rather a method for using search engine queries to refine first approximations from rough models, which we apply to the task of supersense acquisition for illustrative purposes.

We construct a rough approximation using a simplified version of the distributional approach taken by Curran \cite{curran}, and refine using the Bing API.
[GLOSS OVER RESULTS HERE]



\section{Problem Definition and Algorithm}

\subsection{Task Definition}

%Precisely define the problem you are addressing (i.e. formally specify the inputs and outputs). Elaborate on why this is an interesting and important problem. 

The task of supersense acquisition of unknown nouns was first described in \cite{cj}.
The problem is, given a word,  assign to that word one of the WordNet {\it supersenses} (also called {\it lexicographer classes}).
A listing of all the WordNet supersenses may be found in Figure \ref{fig:supersenses}.
\begin{center}
\begin{figure}[hbtp]
\begin{tabular}{llllllll}
1& person & 8& possession&  15& process&  22& body \\
2& communication & 9& location& 16& Tops&  23& feeling \\
3& artifact & 10& substance&  17& phenomenon& 24& shape \\
4& act & 11& state& 18& event&  25& plant \\
5& group & 12& time& 19& quantity&  26& relation \\
6& food & 13&  attribute& 20& motive \\
7& cognition& 14& object& 21& animal \\
\end{tabular}
\caption{List of Supersenses}
\label{fig:supersenses}
\end{figure}
\end{center}

Call a word {\it ambiguous} if it is attested as having more than one supersense in Wordnet.
We follow the literature in restricting ourselves to only attempting to guess supersenses for unambiguous nouns \cite{cj}\cite{curran}.

Supersense information for unknown nouns is conceivably useful for a number of tasks---parsing or POS tagging, for example, could both benefit from supersense information.
Similarly, Word Sense Disambiguation could be aided by supersense information---one could apply selectional restrictions to nouns with guessed supersenses to help disambiguate.
Since lexical resources like WordNet are necessarily incomplete, the utility of being able to guess nominal supersenses is clear.



\subsection{Algorithm Definition}

%Describe in reasonable detail the algorithm you are using to address this problem. A psuedocode description of the algorithm you are using is frequently useful. Trace through a concrete example, showing how your algorithm processes this example. The example should be complex enough to illustrate all of the important aspects of the problem but simple enough to be easily understood. If possible, an intuitively meaningful example is better than one with meaningless symbols. 

The approach we take has a number of steps.
First, we construct a vector space model.
Second, given a word with unknown supersense, we find its nearest $n$ neighbors.
Third, we use search engine queries to refine the model and find the $k$ nearest neighbors (for $k< n$).
Finally, we use a voting scheme over these $k$ nearest neighbors to form a guess of the word's supersense.

\begin{enumerate}
\item\label{modelstep}
We first construct a very simple distributional lexical semantic model.
We process a corpus of raw text and, for each word $w$, count the number of times every other word appears within 5 words of $w$.
Note we use a list of 172 stopwords to completely ignore.
These counts form the basis of our vector space.
We operate on the {\it tf-idf} vectors of these raw count vectors, and use cosine similarity as our distance metric.

Our corpus is approximately 27M words sampled from the English Gigaword Fourth Edition corpus \cite{gigaword}, and we use the Gensim framework \cite{gensim} to construct the vector space.
We chose to use both a small corpus and a fairly simple model due to time constraints---it takes a nontrivial amount of time and disk space to handle larger corpora, and vector space models are not the focus of the current work.

\item \label{enrichstep}
Given a word $w$ with unknown supersense, we will eventually find its $k$ nearest neighbors and use a voting scheme over those to determine supersense.
Before we do that, we first find the $n$ nearest neighbors in the model found in step \ref{modelstep}, for some $n > k$.
There are two options here for which vector we use to represent $w$.
First, we can simply use $w$'s vector as built in step \ref{modelstep}.
Second, we can augment this vector with the results of a search engine query for word $w$, and find the $n$ nearest neighbors for the enriched vector.
We measure the performance of both approaches; the latter outperforms the former.

\item
After establishing the $n$-nearest neighbors, we query the search engine for these $n$ terms, plus the original term. We create a new vector space model using the search results as described in step \ref{modelstep}. Note that this new vector space model only contains $n$-vectors created from search query results, but does not contain any of the data from the original corpus. We then select the $k$-nearest neighbors of this new vector space model to the $w$ vector described in step \ref{enrichstep}.

\item
WORDS ON VOTING

\end{enumerate}

\section{Experimental Evaluation}

\subsection{Methodology}

What are criteria you are using to evaluate your method? What specific hypotheses does your experiment test? Describe the experimental methodology that you used. What are the dependent and independent variables? What is the training/test data that was used, and why is it realistic or interesting? Exactly what performance data did you collect and how are you presenting and analyzing it? Comparisons to competing methods that address the same problem are particularly useful. 

\subsection{Results}

%Present the quantitative results of your experiments. Graphical data presentation such as graphs and histograms are frequently better than tables. What are the basic differences revealed in the data. Are they statistically significant? 

\begin{center}
\begin{figure}[hbtp]
\begin{tabular}{|l| l| l| l|}
\hline
{\bf enrichment?} & {\bf refinement} & {\bf 1.6 P/R/A (F-score)} & {\bf 1.7 P/R/A (F-score)}  \\
\hline \hline
(baseline) & (baseline) & 26.11 / 26.11 / 26.11 (26.11) & 62.60 / 62.60 / 62.60 (62.60) \\
\hline\hline
none & none & 28.75 / 87.34 / 27.60 (43.26) & 27.33 / 76.36 / 25.20 (40.26)  \\
\hline
none & scrape & 34.91 / 52.78 / 26.60 (42.02)  & 29.03 / 32.14 / 18.00 (30.51)\\
\hline
none & descriptions & 34.27 / 78.89 / 31.40 (47.79) &  27.78 / 71.43 / 25.00 (40.00)\\
\hline\hline
scrape & none & 31.94 / 87.93 / 30.60 (46.86) & 21.37 / 67.83 / 19.40 (32.50)  \\
\hline
scrape & scrape & 38.40 / 53.53 / 28.80 (44.72) & 21.50 / 25.48 / 13.20 (23.32) \\
\hline
scrape & descriptions & 40.92 / 79.39 / 37.00 (54.01)  & 21.17 / 62.67 / 18.80 (31.65)  \\
\hline\hline
descriptions & none & 38.43 / 92.08 / 37.20 (54.23) & 19.57 / 78.81 / 18.60 (31.37)  \\
\hline
descriptions & scrape & 43.26 / 59.43 / 33.40 (50.08) & 21.59 / 25.48 / 13.20 (23.32) \\
\hline
descriptions & descriptions & 34.41 / 82.05 / 32.00 (48.48) & 18.67 / 71.90 / 17.40 (29.64)   \\
\hline
\end{tabular}
\caption{Results on 1.6 and 1.7 testsets}
\label{fig:res1}
\end{figure}
\end{center}


\subsection{Discussion}

Is your hypothesis supported? What conclusions do the results support about the strengths and weaknesses of your method compared to other methods? How can the results be explained in terms of the underlying properties of the algorithm and/or the data. 

\section{Related Work}

%Answer the following questions for each piece of related work that addresses the same or a similar problem. What is their problem and method? How is your problem and method different? Why is your problem and method better? 

There are two papers about the particular task of supersense acquisition of nouns.
The first, in which the task is first described, is by Ciaramita \& Johnson \cite{cj}.
They apply a multiclass perceptron classifier which uses a mix of features standardly used in NER, WSD, and lexical disambiguation---parts of speech of neighboring words, $n$-grams in surrounding contexts, and morphological features, for example.
They train on two corpora by, for each unambiguous noun with an entry in WordNet 1.6, calculating a feature vector for each occurrence of that noun.
One corpus is the Bllip corpus, a 40M word corpus of WSJ text; the other is comprised of the definitional glosses collected from WordNet.
The method given in this paper is based entirely on using offline, preprocessed corpora, and augmenting it with results from search engine queries could be fruitful.

The task is approached differently by Curran \cite{curran}.
He uses a more unsupervised approach---construct a distributional vector space model from a large corpus and use a $k$-nearest-neighbors voting scheme to guess the supersense label of an unknown word.
Our basic, offline model is an impoverished version of Curran's---whereas the dimensions in his model correspond to nouns being observed standing in different relations to verbs (e.g. ``being the direct object of {\it give}''), the dimensions in our model correspond simply to nouns being observed within the local contexts of other words.
Our general approach differs from his in that we attempt to also use a search engine---both to augment the initial vector for the unknown word with a greater number of contexts, and also to refine our guess for what the $k$-nearest neighbor vectors are.

Ciaramita and Altun \cite{ciaramita-altun} apply an HMM to the task of WSD on the Supersense level.
That is, instead of guessing supersenses for unknown nouns, they attempt to label individual occurrences of words with appropriate supersenses.
This is a significantly more complicated problem than the one addressed here, and it is not at all obvious how one could use information from the web to help in the task.

%Erk \cite{erk} gives an approach to the somewhat related problem of detecting unknown word senses---automatically detecting occurrences of words in text whose senses are not in a given sense inventory.

The idea of using the web to build models for natural language processing tasks is certainly not novel.
For example, \cite{bollegala} and \cite{gracia} use the results of search engine queries to calculate semantic similarity between terms.
\cite{strube} and \cite{gabrilovich} also use the web to calculate semantic similarity, but restrict themselves to Wikipedia.
However, as far as we know, there is no prior work using the web to guess supersenses of unknown words.
Further, we believe use of search engine queries to refine an initial approximate answer provided by an offline model (in contrast to building a model from the web) is not a widely used technique.
This is the primary contribution of the present work.

\section{Future Work}

%What are the major shortcomings of your current method? For each shortcoming, propose additions or enhancements that would help overcome it. 

[SHORTCOMINGS OF METHOD, POSSIBLE ENHANCEMENTS]

The general approach we take is to construct a rough first approximation of an answer from a relatively small offline corpus and then use a search engine to refine this approximation.
This approach could be fruitful for a number of different tasks---guessing word similarity or automatically finding synonyms, for example.
Any task whose process of constructing a solution admits refinement with a search engine  could potentially see improvement using this method.

\section{Conclusion}

Briefly summarize the important results and conclusions presented in the paper. What are the most important points illustrated by your work? How will your results improve future research and applications in the area? 




\bibliography{writeup}{}
\bibliographystyle{plain}


\end{document}